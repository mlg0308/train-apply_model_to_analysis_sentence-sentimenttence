%matplotlib inline
import numpy as np
import warnings
warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')
from gensim.models import KeyedVectors                                                  
import random
import re
import matplotlib.pyplot as plt                                  
from sklearn.model_selection import train_test_split
import os
import jieba
import jieba.analyse
import jieba.posseg as psg
from collections import Counter

cn_model = KeyedVectors.load_word2vec_format(
    'E:/Chinese-Word-Vectors-master/sgns.renmin.bigram-char', 
    binary = False)         #基于《人民日报》的word2vector
#cn_model = KeyedVectors.load_word2vec_format(
#'E:/Chinese-Word-Vectors-master/merge_sgns_bigram_char300.txt',
#binary = False)            #基于混合中文语料的word2vector
#
#Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, Xiaoyong Du, 
#   Analogical Reasoning on Chinese Morphological and Semantic Relations, ACL 2018.
#
#@InProceedings{P18-2023,
#  author =  "Li, Shen
#    and Zhao, Zhe
#    and Hu, Renfen
#    and Li, Wensi
#    and Liu, Tao
#    and Du, Xiaoyong",
#  title =   "Analogical Reasoning on Chinese Morphological and Semantic Relations",
#  booktitle =   "Proceedings of the 56th Annual Meeting of the 
#  Association for Computational Linguistics (Volume 2: Short Papers)",
#  year =  "2018",
#  publisher =   "Association for Computational Linguistics",
#  pages =   "138--143",
#  location =  "Melbourne, Australia",
#  url =   "http://aclweb.org/anthology/P18-2023"
#  }


#加载训练样本
fp = open ('__yourpath__','r' , encoding = 'utf-8')
fn = open ('__yourpath__','r' , encoding = 'utf-8')
train_txt_orig = []
neg = []
pos = []
for i in fp:
    train_txt_orig.append(i)
    pos.append(i)
for i in fn:
    train_txt_orig.append(i)    
    neg.append(i)
fp.close()
fn.close()
print('总共训练样本量是： ' + str(len(train_txt_orig)))
print('正面训练样本量是： ' + str(len(pos)))
print('负面训练样本量是： ' + str(len(neg)))


#分词、去除标点符号
def cut_txt(txt):
    txtlist = []
    dots = ['。','，','！','？','、','：','；','”','“','’','‘','\'','"','（',\
            '）','——','—','(',')','……','…','@','#','%','&','·','*','=','=',\
            '-','\\','/','<','>','《','》','[',']','|','|','￥','$','\s+',' ','"']
    list = jieba.cut(txt)
    for i in list:
        if not i in dots:
            txtlist.append(i)
    return txtlist
  
 
 #对分词后的语料进行word-embedding
#定义训练集
train_tokens = []
for i in train_txt_orig:
    #句子分词去除标点后生成一个列表，形如“['学习','python','使','我','快乐']”
    senlist = cut_txt(i)
    cut_list = []
    for i in senlist:
        #获取索引并生成list
        try:
            i_index = cn_model.vocab[i].index
        except KeyError:
            i_index = 0
        cut_list.append(i_index)
    train_tokens.append(cut_list)

#生成一个以子list长度为元素的向量
num_tokens = [len(tokens) for tokens in train_tokens]
num_tokens = np.array(num_tokens)

#训练语料少，直接将语料中得最长值定义为max_tokens
max_tokens = np.max(num_tokens)


from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense,GRU,Embedding,LSTM,Bidirectional
from tensorflow.python.keras.preprocessing.text import Tokenizer
from tensorflow.python.keras.preprocessing.sequence import pad_sequences
from tensorflow.python.keras.optimizers import RMSprop
from tensorflow.python.keras.optimizers import Adam
from tensorflow.python.keras.callbacks import EarlyStopping,ModelCheckpoint,TensorBoard,ReduceLROnPlateau

embedding_dim = cn_model['私营企业'].shape[0]
#预训练的词向量模型共收录了350000+词汇，在这里我么使用其前350000个词
#训练语料少，尽可能多从cn_model获取信息
#训练语料与生成cn_model的语料同源，理论上训练语料中的所有词都能在cn_model中找到
num_words = 350000
#构造初始化的embedding_matrix备用
embedding_matrix = np.zeros((num_words,embedding_dim))
for i in range(num_words):
    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]
#embedding_matrix必须是float32,不然会出错
embedding_matrix = embedding_matrix.astype('float32')

#max_tokens = 114

#对语料进行裁剪、填充
train_pad = pad_sequences(train_tokens,maxlen = max_tokens,
                         padding = 'pre' , truncating = 'pre')
train_pad[train_pad >= num_words] = 0

#构造训练目标
a = []
for i in range(1604):
    a.append(1)
for i in range(552):
    a.append(0)
a = np.array(a)
train_target = a

#训练集、测试集分割
X_train,X_test,y_train,y_test = train_test_split(train_pad,train_target,
                                                 test_size = 0.1,random_state=5)                                                

#搭建训练模型
model = Sequential()
model.add(Embedding(num_words,embedding_dim,weights=[embedding_matrix],
                    input_length=max_tokens,trainable=False))
model.add(Bidirectional(LSTM(units=32,return_sequences=True)))
model.add(LSTM(units=16,return_sequences=False))
model.add(Dense(1,activation = 'sigmoid'))
#模型优化
optimizer = Adam(lr=1e-3)
model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])
#模型汇总
model.summary()

path_checkpoint = 'sentiment_checkpoint.keras'
checkpoint = ModelCheckpoint(filepath = path_checkpoint,monitor='val_loss',
                            verbose=1,save_weights_only=True,
                            save_best_only=True)
try:
    model.load_weights(path_checkpoint)
except Exception as e:
    print(e)
earlystopping = EarlyStopping(monitor='val_loss',patience=3,verbose=1)
lr_reduction = ReduceLROnPlateau(monitor='val_loss',factor=0.1,
                                 min_lr=1e-5,patience=1,verbose=1)
callbacks = [
    earlystopping,
    checkpoint,
    lr_reduction
]



#开始训练
model.fit(X_train,y_train,
         validation_split = 0.1,
         epochs = 20,
         batch_size=64,
         callbacks=callbacks )

result = model.evaluate(X_test,y_test)
print('Accuracy:{0:.4%}'.format(result[1]))


def predict_sentiment(txt):
    senlist = cut_txt(txt)
    cut_list = []
    for i in senlist:
        try:
            i_index = cn_model.vocab[i].index
        except KeyError:
            i_index = 0
        cut_list.append(i_index)
    tokens_pad = pad_sequences([cut_list],maxlen = max_tokens,padding='pre',truncating='pre')
    
    result = model.predict(x=tokens_pad)
    coef = result[0][0]
    return coef
    #print(coef)
    
    
    
predict_sentiment('__your_target_sentence__')
